{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:45:32.479142Z",
     "start_time": "2019-09-29T08:45:32.233067Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TCEH7UbCkTv_"
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:45:32.561318Z",
     "start_time": "2019-09-29T08:45:32.481202Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bkGglN-vkTph",
    "outputId": "11b85f53-6171-4d63-a46f-312d02781e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1htbfPDkzjx4"
   },
   "source": [
    "\n",
    "# Implementation of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mg87IEOB0Exc"
   },
   "source": [
    "## Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:49:29.102753Z",
     "start_time": "2019-09-29T08:49:29.096121Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1898XPru6C9m",
    "outputId": "a3c452ca-cf1e-4cc8-80c1-59c58bb99230"
   },
   "outputs": [],
   "source": [
    "# Read stop word\n",
    "def read_stop_words(remove_punc=False):\n",
    "    filepath = \"data/english.stop\"\n",
    "    stop_words = []\n",
    "    with open(filepath) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            if remove_punc:\n",
    "                line = re.sub(r\"[^a-z]\", \"\", line) #remove punctuation            \n",
    "            stop_words.append(line) \n",
    "            cnt += 1\n",
    "            line = fp.readline()\n",
    "    print(\"There are\", cnt, \"stop words\")\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:49:30.116339Z",
     "start_time": "2019-09-29T08:49:30.108533Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0DlwYt-akr5J"
   },
   "outputs": [],
   "source": [
    "def tokenize(s):    \n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z]\", \" \", s) # remove punctuation \n",
    "    ws = re.compile(r\"\\s+\").split(s)    \n",
    "    return ws\n",
    "\n",
    "def count_words(texts, stop_words=[]):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parametersl\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = set()\n",
    "    for text in texts:      \n",
    "        ws = tokenize(text)      \n",
    "        for w in ws:   \n",
    "            if len(w) > 0 and w not in stop_words:\n",
    "                words.add(w)\n",
    "    \n",
    "    vocabulary = dict()\n",
    "    for i, word in enumerate(words):\n",
    "        vocabulary[word] = i\n",
    "        \n",
    "    n_features = len(vocabulary)\n",
    "    print(\"Number of words in vocabulary:\", n_features)\n",
    "    counts = np.zeros((len(texts), n_features))\n",
    "    for i, text in enumerate(texts):\n",
    "        ws = tokenize(text)\n",
    "        for word in ws: \n",
    "            if word in vocabulary:\n",
    "                word_index = vocabulary[word]\n",
    "                counts[i, word_index] += 1\n",
    "    return vocabulary, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:49:48.168988Z",
     "start_time": "2019-09-29T08:49:31.110270Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "56J5ljlslOAE",
    "outputId": "ab5e02b5-e7c4-48c3-c1f4-1187df1e4087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 572 stop words\n",
      "Number of words in vocabulary: 38395\n",
      "X shape (2000, 38395)\n"
     ]
    }
   ],
   "source": [
    "stop_words = read_stop_words(remove_punc=True)\n",
    "vocabulary, X = count_words(texts, stop_words=stop_words)\n",
    "print(\"X shape\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(y)))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "texts = np.array(texts)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:51:06.746491Z",
     "start_time": "2019-09-29T08:51:06.734387Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NNrNdrpYl09P"
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):        \n",
    "        self.prior = []\n",
    "        self.likelihood = []\n",
    "        self.classes = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        prior = [0] * len(self.classes)\n",
    "        N = len(y)\n",
    "        likelihood = np.zeros((X.shape[1], len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            prior[i] = np.sum(y == c) / N \n",
    "            X_class = np.sum(X[y == c], axis=0) # count number of occurences of token in each class        \n",
    "            likelihood[:, i] = X_class        \n",
    "        likelihood = likelihood + 1\n",
    "        likelihood = likelihood / np.sum(likelihood, axis=0).reshape(1, -1)\n",
    "          \n",
    "        self.prior = prior\n",
    "        self.likelihood = likelihood\n",
    "                    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):                \n",
    "        scores = X @ np.log(self.likelihood) + np.log(self.prior).reshape(1, -1)\n",
    "        y_pred = np.argmax(scores, axis=1)        \n",
    "        return [self.classes[i] for i in y_pred]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUADJeNM5mmU"
   },
   "source": [
    "# Performance evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    return np.mean(y == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBL_mXf11OHJ"
   },
   "outputs": [],
   "source": [
    "def cross_validation(clf, X, y, n_folds=5):\n",
    "    interval = len(y) // n_folds\n",
    "    scores = []\n",
    "    for i in range(n_folds):\n",
    "        start = int(i * interval)\n",
    "        end = int((i + 1) * interval)\n",
    "\n",
    "        X_test = X[start:end]\n",
    "        y_test = y[start:end]\n",
    "        X_train = np.concatenate([X[:start], X[end:]])\n",
    "        y_train = np.concatenate([y[:start], y[end:]])\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        score = accuracy(y_pred, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation of NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C2J6qCuI28jk",
    "outputId": "7fe798dd-aec3-450f-c93a-85bce19aa6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.785, 0.8175, 0.79, 0.795, 0.8075] 0.799\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "scores = cross_validation(nb, X, y)\n",
    "print(scores, np.mean(scores)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NScnoJcN3sue",
    "outputId": "994aa102-1628-4606-a129-f2aa82dd035d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.785, 0.8175, 0.79, 0.795, 0.8075] 0.799\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "scores = cross_validation(mnb, X, y)\n",
    "print(scores, np.mean(scores)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use sklearn and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(clf, analyzer):\n",
    "    mnb_pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words='english', analyzer=analyzer)),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    scores = cross_validation(mnb_pipeline, texts, y)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb_pipeline = create_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.785, 0.815, 0.7925, 0.795, 0.815] 0.8004999999999999\n"
     ]
    }
   ],
   "source": [
    "mnb_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_validation(mnb_pipeline, texts, y)\n",
    "print(scores, np.mean(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.785, 0.815, 0.7925, 0.795, 0.815] 0.8004999999999999\n"
     ]
    }
   ],
   "source": [
    "nb_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('clf', NB()),\n",
    "])\n",
    "scores = cross_validation(nb_pipeline, texts, y)\n",
    "print(scores, np.mean(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLPlab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
